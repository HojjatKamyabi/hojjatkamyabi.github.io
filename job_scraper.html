<!DOCTYPE HTML>
<!--
	Miniport by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Job Skill Trends</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<link rel="icon" href="/Icon.png">
		<link rel="preconnect" href="https://fonts.googleapis.com">
		<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
		<link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;600;700&family=Roboto+Slab:wght@400;700&display=swap" rel="stylesheet">
		<style>
			:root {
				--accent-color: #4361ee;
				--heading-color: #1d3557;
				--text-color: #5c62a3;
				--light-bg: #f8f9fa;
			}
			
			body {
				font-family: 'Poppins', sans-serif;
				line-height: 1.7;
				color: var(--text-color);
			}
			
			h1, h2, h3, h4, .container h1 {
				font-family: 'Roboto Slab', serif;
				color: var(--heading-color);
				font-weight: 700;
				letter-spacing: -0.02em;
				margin-top: 1.5em;
				margin-bottom: 0.8em;
			}
			
			h1 {
				font-size: 2.8rem;
				position: relative;
			}
			
			h1::after {
				content: "";
				display: block;
				width: 80px;
				height: 4px;
				background: var(--accent-color);
				margin-top: 0.5rem;
			}
			
			h2 {
				font-size: 1.8rem;
				border-left: 4px solid var(--accent-color);
				padding-left: 15px;
			}
			
			h3 {
				font-size: 1.3rem;
				color: var(--accent-color);
			}
			
			p {
				margin-bottom: 1.5rem;
				font-weight: 300;
				font-size: 1.05rem;
			}
			
			ul {
				padding-left: 1.2rem;
			}
			
			li {
				margin-bottom: 0.5rem;
			}
			
			strong {
				font-weight: 600;
				color: var(--heading-color);
			}
			
			a {
				color: var(--accent-color);
				text-decoration: none;
				transition: all 0.2s ease;
				font-weight: 500;
			}
			
			a:hover {
				color: #2d3fe1;
				text-decoration: underline;
			}
			
			.container {
				max-width: 1100px;
				padding: 0 2rem;
			}
			
			/* Keep original nav styling */
			#nav {
				font-family: inherit; /* Use the original font from main.css */
			}
			
			#nav ul li {
				font-family: inherit; /* Use the original font from main.css */
			}
			
			#nav ul li a {
				font-family: inherit; /* Use the original font from main.css */
			}
			
			.section {
				padding: 2rem 0;
				margin-bottom: 2rem;
				border-radius: 8px;
				background-color: var(--light-bg);
				padding: 1.5rem;
			}
			
			.section h3 {
				margin-top: 1.2rem;
			}
			
			img {
				border-radius: 6px;
				box-shadow: 0 10px 20px rgba(0,0,0,0.1);
			}
			
			code {
				background: #e9ecef;
				padding: 0.2em 0.4em;
				border-radius: 3px;
				font-family: monospace;
				font-size: 0.9em;
			}
		</style>
		
	</head>
	<body class="is-preload">
		
		<!-- Nav -->
		<nav id="nav">
			<ul class="container">
				<li><a href="index.html">Home</a></li>
				<li> | </li>
				<li> contact me :</li>
				<li><a target="_blank" rel="noreferrer noopener" href="https://www.linkedin.com/in/hojjat-kamyabi-474595319/" class="icon brands fa-linkedin-in"><span class="label">LinkedIn</span></a></li>
				<li><a target="_blank" rel="noreferrer noopener" href="https://github.com/HojjatKamyabi" class="icon brands fa-github"><span class="label">Github</span></a></li>
				
			</ul>
		</nav>

        <!-- Home -->
		<article id="top" class="wrapper style1">
			<div class="container">

                <h1>Tracking Job Skill Trends in Germany</h1>
                <p><a target="_blank" rel="noreferrer noopener" href="https://github.com/HojjatKamyabi/Linkedin_Market_Analysis">View on GitHub</a></p>
                <img src="./images/job_da.jpg" alt="Power BI dashboard of job skill trends" style="max-width: 100%; height: auto;">
                <br><br>

                <p>This project is about understanding what tech skills companies are looking for in different parts of Germany. I used data from LinkedIn job posts and turned it into an interactive map that shows which skills are most popular in each region.</p>

                <h2>What's the Goal?</h2>
                <p>To help job seekers and recruiters see which tech skills are in high demand based on location, so they can make better decisions about where to apply, hire, or upskill.</p>

                <h2>How It Works</h2>
                <p>The project has three main steps:</p>
                <ul>
                    <li><strong>Collect Data:</strong> I built a Python web scraper to pull job postings from LinkedIn. It saves the job title, description, location, and more.</li>
                    <li><strong>Extract Skills:</strong> Using text analysis, the scraper detects tech skills mentioned in each job description. I used tools like regex and fuzzy matching (RapidFuzz) to make it accurate.</li>
                    <li><strong>Visualize the Results:</strong> All the data is stored in PostgreSQL and connected to a Power BI dashboard. The map shows how popular each skill is by city or state. When you hover over a region, it shows the top skill and its count.</li>
                </ul>

                <h2>Tools I Used</h2>
                <ul>
                    <li><strong>Python</strong> – for scraping and cleaning data</li>
                    <li><strong>BeautifulSoup & Selenium</strong> – to extract content from job pages</li>
                    <li><strong>Apache Airflow</strong> – to automate the whole process on a schedule</li>
                    <li><strong>Docker</strong> – to containerize everything so it runs smoothly</li>
                    <li><strong>PostgreSQL</strong> – as the database for all cleaned job and skill data</li>
                    <li><strong>Power BI</strong> – to build the interactive dashboard</li>
                </ul>

                <br>
                
                <h2>The Main Scraper</h2>
                <p>The script scrapes <strong>LinkedIn Jobs</strong> to find job postings based on a specific role (e.g., data analyst) and location (e.g., Germany).</p>

                <h3>Which tools are used?</h3>
                <p>
                This project uses two main tools:
                <ul>
                    <li><strong>Selenium</strong> to automate the browser and interact with job elements</li>
                    <li><strong>BeautifulSoup</strong> to parse and clean job descriptions from HTML</li>
                </ul>
                </p>

                <h3>What data is extracted?</h3>
                <p>The scraper collects the following information from each job listing:</p>
                <ul>
                <li>Job title</li>
                <li>Company name</li>
                <li>Location</li>
                <li>Date posted</li>
                <li>Full job description</li>
                <li>Job link</li>
                <li>Scrape date and time</li>
                </ul>

                <h3>How are pagination, delays, and anti-bot protection handled?</h3>
                <p>
                The script scrolls the page several times to load more job listings, simulating real user behavior. It uses time delays (3 seconds) between actions to reduce the chance of being blocked. Each job link is opened in a new browser tab, data is extracted, and the tab is closed, mimicking natural browsing patterns.
                </p>

                <h3>Where is the data stored?</h3>
                <p>
                All extracted job data is saved into a <strong>PostgreSQL database</strong> using the <code>psycopg2</code> library. The script avoids duplicate entries by checking if the job link already exists before inserting. (remember to change the database connection string in the code to your own database)
                </p>
                <br>

                <h2>Airflow Dags</h2>
                <div class="section">
                    <h3>LinkedIn Scraper DAG</h3>

                    <h4>What does this DAG do?</h4>
                    <p>
                    This DAG runs the Python script that scrapes job data from LinkedIn and stores it in a PostgreSQL database.
                    </p>

                    <h4>How often does it run?</h4>
                    <p>
                    This DAG is set to run manually (no schedule is defined), but it can be triggered on demand from the Airflow UI.
                    </p>

                    <h4>What are its tasks?</h4>
                    <p>
                    It has one task:
                    <ul>
                        <li><strong>run_scraper</strong>: This task runs a Bash command that changes the directory to the scraper folder and executes <code>main.py</code>, the main scraping script.</li>
                    </ul>
                    </p>

                    <h4>Are there any dependencies?</h4>
                    <p>
                    No task dependencies in this DAG since it only has one task.
                    </p>

                    <h4>Any special handling or error checking?</h4>
                    <p>
                    The DAG has basic retry logic: if the task fails, it will retry once after a 5-minute delay.
                    </p>
                </div>
                <br>

                <div class="section">
                    <h3>LinkedIn Skills Extraction DAG</h3>

                    <h4>What does this DAG do?</h4>
                    <p>
                    This DAG reads job descriptions from a PostgreSQL database and extracts relevant data analyst skills using fuzzy matching.
                    It then updates each job entry with the detected skills and saves the top 20 most frequent skills to a separate table.
                    </p>

                    <h4>How often does it run?</h4>
                    <p>
                    The DAG runs automatically once a week.
                    </p>

                    <h4>What are its tasks?</h4>
                    <p>
                    It has one task:
                    <ul>
                        <li><strong>extract_skills</strong>: A Python function that connects to the database, analyzes job descriptions, detects key skills based on a predefined list, and updates the database with the results.</li>
                    </ul>
                    </p>

                    <h4>Are there any dependencies?</h4>
                    <p>
                    No task dependencies, as the DAG includes only one task.
                    </p>

                    <h4>Any special handling or error checking?</h4>
                    <p>
                    The DAG will retry the task up to 2 times if it fails, with a 5-minute delay between retries. The skill matching uses a fuzzy logic threshold to ensure relevant results.
                    </p>
                </div>
                <br>

                <div class="section" id="docker">
                    <h2>Docker & Containerization</h2>
                    <p>To ensure a consistent, reproducible, and isolated environment for the project, Docker was used to containerize all components involved in scraping, scheduling, and storing data.</p>
                
                    <h3>Components</h3>
                    <ul>
                    <li><strong>PostgreSQL:</strong> A dedicated container runs PostgreSQL 15, acting as the central database to store the scraped job data.</li>
                    <li><strong>Scraper:</strong> Built with a minimal <code>python:3.10-slim</code> base image, this container runs a Python script that scrapes job listings using Selenium and Chrome in headless mode.</li>
                    <li><strong>Apache Airflow:</strong> Two separate containers for the Airflow <strong>webserver</strong> and <strong>scheduler</strong> were created, both customized to support scraping tasks and orchestration of jobs.</li>
                    </ul>
                
                    <h3>Technologies & Tools</h3>
                    <ul>
                    <li>Custom <code>Dockerfile</code> for both the scraper and Airflow containers to install dependencies, Chrome, and ChromeDriver.</li>
                    <li><code>docker-compose.yml</code> to manage multi-container setup, volumes, environment variables, and service dependencies.</li>
                    <li>Mounted shared volumes to allow Airflow and scraper containers to access the same codebase.</li>
                    <li>All containers connected via a custom Docker bridge network <code>scraper_network</code> for seamless internal communication.</li>
                    </ul>
                
                    <h3>Highlights</h3>
                    <ul>
                    <li><strong>Airflow LocalExecutor:</strong> Ensures task execution without the need for an external worker setup.</li>
                    <li><strong>airflow_init service:</strong> Handles database migrations and user creation automatically on first run.</li>
                    <li><strong>Headless Chrome:</strong> Installed in both scraper and Airflow containers to run Selenium without UI.</li>
                    <li><strong>Resource Management:</strong> Memory limits and reservations defined for Airflow containers to ensure stability.</li>
                    </ul>
                
                    <h3>Benefits</h3>
                    <ul>
                    <li>Reproducible environments across any system</li>
                    <li>Isolated containers for better testing and maintenance</li>
                    <li>Easily deployable and scalable architecture</li>
                    </ul>
                </div>
                

                <br>
                <h2>What You Can See in the PowerBI Dashboard</h2>
                <ul>
                    <li>KPIs, including most in-demand skill across all states, total skills extracted, and last scrape time.</li>
                    <li>A map of Germany showing demand for skills like Python, SQL, AWS, etc. in different states.</li>
                    <li>Hovering over a location shows the most requested skill and how many times it appeared.</li>
                </ul>

                <h2>Why This Matters</h2>
                <p>Understanding job market trends helps professionals focus on the right skills and find better job opportunities. This tool makes that easier by showing the data in a simple, visual way.</p>

                <h2>See the Code</h2>
                <p>All code and setup instructions are available on <a target="_blank" href="https://github.com/HojjatKamyabi/Linkedin_Market_Analysis">GitHub</a>. You can even run the whole project locally using Docker.</p>

            </div>
		</article>

    </body>
    </html>
